{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import regex as re\n",
    "import subprocess\n",
    "import urllib\n",
    "import functools\n",
    "\n",
    "import torch.distributions.distribution\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Graph import PeriodicPlotter\n",
    "from LSTM_Model import *\n",
    "from MySong import *\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    print(\"GPU is available, Switching now.\")\n",
    "    torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "train = False\n",
    "inference = True\n",
    "### Hyperparameter setting and optimization ###\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "# Optimization parameters:\n",
    "num_training_iterations = 10000  # Increase this to train longer\n",
    "batch_size = 4  # Experiment between 1 and 64\n",
    "seq_length = 100  # Experiment between 50 and 500\n",
    "learning_rate = 5e-2  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Model parameters:\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location:\n",
    "checkpoint_dir = 'training_checkpoints_pytorch'\n",
    "checkpoint_prefix = 'my_ckpt'\n",
    "\n",
    "checkpoint_dir = os.path.join(cwd, checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, checkpoint_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 818 songs in text\n",
      "\n",
      "Example song: \n",
      "X:1\n",
      "T:Alexander's\n",
      "Z: id:dc-hornpipe-1\n",
      "M:C|\n",
      "L:1/8\n",
      "K:D Major\n",
      "(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\n",
      "dAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\n",
      "AG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\n",
      "FAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\pecko\\\\PycharmProjects\\\\MusicGenerator\\\\outputs\\\\output@FriMar110117342022\\\\sampleFromDataset.abc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_15908/1438831760.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# need abc2midi and timidity\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[0mplay_song\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexample_song\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\MusicGenerator\\MySong.py\u001B[0m in \u001B[0;36mplay_song\u001B[1;34m(song)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mplay_song\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msong\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m     \u001B[0mbasename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_song_to_abc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msong\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m     \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mabc2wav\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbasename\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m'.abc'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# did not suceed\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\MusicGenerator\\MySong.py\u001B[0m in \u001B[0;36msave_song_to_abc\u001B[1;34m(song, filename)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0msave_song_to_abc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msong\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"sampleFromDataset\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[0msave_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"{}.abc\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msave_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"w\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msong\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pecko\\\\PycharmProjects\\\\MusicGenerator\\\\outputs\\\\output@FriMar110117342022\\\\sampleFromDataset.abc'"
     ]
    }
   ],
   "source": [
    "songs = []\n",
    "with open(os.path.join(cwd, 'dataset', 'irish.abc'), 'r') as f:\n",
    "    text = f.read()\n",
    "    songs = extract_song_snippet(text)\n",
    "\n",
    "# Print one of the songs to inspect it in greater detail!\n",
    "example_song = songs[0]\n",
    "print(\"\\nExample song: \")\n",
    "print(example_song)\n",
    "\n",
    "# need abc2midi and timidity\n",
    "play_song(example_song)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join our list of song strings into a single string containing all songs\n",
    "songs_joined = \"\\n\\n\".join(songs)\n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(songs_joined))\n",
    "vocab_size = len(vocab)\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")\n",
    "\n",
    "### Define numerical representation of text ###\n",
    "\n",
    "# Create a mapping from character to unique index.\n",
    "# For example, to get the index of the character \"d\",\n",
    "#   we can evaluate `char2idx[\"d\"]`.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from indices to characters. This is\n",
    "#   the inverse of char2idx and allows us to convert back\n",
    "#   from unique index to the character in our vocabulary.\n",
    "idx2char = np.array(vocab)  ### Define numerical representation of text ###\n",
    "\n",
    "# Create a mapping from character to unique index.\n",
    "# For example, to get the index of the character \"d\",\n",
    "#   we can evaluate `char2idx[\"d\"]`.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from indices to characters. This is\n",
    "#   the inverse of char2idx and allows us to convert back\n",
    "#   from unique index to the character in our vocabulary.\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "print('{')\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "### Vectorize the songs string ###\n",
    "\n",
    "'''TODO: Write a function to convert the all songs string to a vectorized\n",
    "    (i.e., numeric) representation. Use the appropriate mapping\n",
    "    above to convert from vocab characters to the corresponding indices.\n",
    "\n",
    "  NOTE: the output of the `vectorize_string` function\n",
    "  should be a np.array with `N` elements, where `N` is\n",
    "  the number of characters in the input string\n",
    "'''\n",
    "\n",
    "\n",
    "def vectorize_string(string):\n",
    "    vectorized_list = np.array([char2idx[s] for s in string])\n",
    "    return vectorized_list\n",
    "\n",
    "\n",
    "vectorized_songs = vectorize_string(songs_joined)\n",
    "\n",
    "print('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
    "# check that vectorized_songs is a numpy array\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\"\n",
    "\n",
    "\n",
    "def test_batch_func_types(func, args):\n",
    "    ret = func(*args)\n",
    "    assert len(ret) == 2, \"[FAIL] get_batch must return two arguments (input and label)\"\n",
    "    assert type(ret[0]) == np.ndarray, \"[FAIL] test_batch_func_types: x is not np.array\"\n",
    "    assert type(ret[1]) == np.ndarray, \"[FAIL] test_batch_func_types: y is not np.array\"\n",
    "    print(\"[PASS] test_batch_func_types\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_batch_func_shapes(func, args):\n",
    "    dataset, seq_length, batch_size = args\n",
    "    x, y = func(*args)\n",
    "    correct = (batch_size, seq_length)\n",
    "    assert x.shape == correct, \"[FAIL] test_batch_func_shapes: x {} is not correct shape {}\".format(x.shape, correct)\n",
    "    assert y.shape == correct, \"[FAIL] test_batch_func_shapes: y {} is not correct shape {}\".format(y.shape, correct)\n",
    "    print(\"[PASS] test_batch_func_shapes\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_batch_func_next_step(func, args):\n",
    "    x, y = func(*args)\n",
    "    assert (x[:, 1:] == y[:, :-1]).all(), \"[FAIL] test_batch_func_next_step: x_{t} must equal y_{t-1} for all t\"\n",
    "    print(\"[PASS] test_batch_func_next_step\")\n",
    "    return True\n",
    "\n",
    "\n",
    "### Batch definition to create training examples ###\n",
    "\n",
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    # the length of the vectorized songs string\n",
    "    n = vectorized_songs.shape[0] - 1\n",
    "    # randomly choose the starting indices for the examples in the training batch\n",
    "    idx = np.random.choice(n - seq_length, batch_size)\n",
    "\n",
    "    '''TODO: construct a list of input sequences for the training batch'''\n",
    "    input_batch = [vectorized_songs[i:i + seq_length] for i in idx]\n",
    "    '''TODO: construct a list of output sequences for the training batch'''\n",
    "    output_batch = [vectorized_songs[i + 1: i + 1 + seq_length] for i in idx]\n",
    "\n",
    "    # x_batch, y_batch provide the true inputs and targets for network training\n",
    "    x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
    "    return x_batch, y_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform some simple tests to make sure your batch function is working properly!\n",
    "test_args = (vectorized_songs, 10, 2)\n",
    "if not test_batch_func_types(get_batch, test_args) or \\\n",
    "        not test_batch_func_shapes(get_batch, test_args) or \\\n",
    "        not test_batch_func_next_step(get_batch, test_args):\n",
    "    print(\"======\\n[FAIL] could not pass tests\")\n",
    "else:\n",
    "    print(\"======\\n[PASS] passed all tests!\")\n",
    "\n",
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a simple model with default hyperparameters. You will get the\n",
    "#   chance to change these later.\n",
    "#model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=4)\n",
    "#model = MusicGenerator(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=4, seq_length=100)\n",
    "model = MyLSTM(vocab_size, embedding_dim, rnn_units, batch_size, seq_length)\n",
    "print(model)\n",
    "\n",
    "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=4)\n",
    "\n",
    "hn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "cn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "pred, (hn, cn) = model(torch.tensor(x), hn, cn)\n",
    "#pred = model(x)\n",
    "\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "sampled_indices = torch.distributions.categorical.Categorical(logits=pred[0]).sample()\n",
    "sampled_indices = torch.squeeze(sampled_indices, dim=-1).numpy()\n",
    "\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
    "# print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Defining the loss function ###\n",
    "\n",
    "'''TODO: define the loss function to compute and return the loss between\n",
    "    the true labels and predictions (logits). Set the argument from_logits=True.'''\n",
    "\n",
    "\n",
    "def compute_loss(labels, logits):\n",
    "    #loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits.detach().numpy(), from_logits=True)\n",
    "    x = torch.Tensor(logits).permute((0, 2, 1))  # shape of preds must be (N, C, H, W) instead of (N, H, W, C)\n",
    "    y = torch.Tensor(labels).long()  # shape of labels must be (N, H, W) and type must be long integer\n",
    "    loss = torch.nn.CrossEntropyLoss()(x, y)\n",
    "    return loss\n",
    "\n",
    "'''TODO: compute the loss using the true next characters from the example batch\n",
    "    and the predictions from the untrained model several cells above'''\n",
    "example_batch_loss = compute_loss(y, pred)\n",
    "\n",
    "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "'''\n",
    "example_batch_loss.numpy().mean() = 4.417909\n",
    "'''\n",
    "print(\"scalar_loss:      \", example_batch_loss.detach().numpy().mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(cwd, checkpoint_dir)\n",
    "#try to create pytorch training checkpoints directory\n",
    "try:\n",
    "    os.mkdir(checkpoint_dir)\n",
    "except FileExistsError:\n",
    "    print(\"The pytorch training directory already exists...\")\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, checkpoint_prefix)\n",
    "\n",
    "\n",
    "### Define optimizer and training operation ###\n",
    "\n",
    "'''instantiate a new model for training using the `build_model`\n",
    "  function and the hyperparameters created above.'''\n",
    "model = MyLSTM(vocab_size, embedding_dim, rnn_units, batch_size, seq_length)\n",
    "\n",
    "''' instantiate an optimizer with its learning rate.\n",
    "  Checkout the tensorflow website for a list of supported optimizers.\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
    "  Try using the Adam optimizer to start.'''\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "def torch_train(x, y, hn, cn):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass and loss calculation\n",
    "    # implicit tape-based AD\n",
    "    y_hat, (hn, cn) = model(torch.tensor(x), hn, cn)\n",
    "\n",
    "    loss = compute_loss(y, y_hat)\n",
    "\n",
    "    # compute gradients (grad)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, (hn, cn)\n",
    "\n",
    "### Prediction of a generated song ###\n",
    "\n",
    "def generate_text(model, start_string, generation_length=1000):\n",
    "    # Evaluation step (generating ABC text using the learned RNN model)\n",
    "\n",
    "    '''TODO: convert the start string to numbers (vectorize)'''\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    #model.reset_states()\n",
    "    tqdm._instances.clear()\n",
    "\n",
    "    hn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "    cn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "    for i in tqdm(range(generation_length)):\n",
    "        #predictions = model(input_eval)\n",
    "        predictions, (hn, cn) = model(input_eval, hn, cn)\n",
    "\n",
    "        # Remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        '''TODO: use a multinomial distribution to sample'''\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the prediction along with the previous hidden state\n",
    "        #   as the next inputs to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        '''TODO: add the predicted character to the generated text!'''\n",
    "        # Hint: consider what format the prediction is in vs. the output\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "if train:\n",
    "\n",
    "    # try to create pytorch training checkpoints directory\n",
    "    try:\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    except FileExistsError:\n",
    "        print(\"The pytorch training directory already exists...\")\n",
    "\n",
    "    ##################\n",
    "    # Begin training!#\n",
    "    ##################\n",
    "    history = []\n",
    "    plotter = PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "    if hasattr(tqdm, '_instances'): tqdm._instances.clear()  # clear if it exists\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "        cn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "\n",
    "        for iter in tqdm(range(num_training_iterations)):\n",
    "\n",
    "            # Grab a batch and propagate it through the network\n",
    "            x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
    "            loss, (hn, cn) = torch_train(x_batch, y_batch, hn, cn)\n",
    "\n",
    "            # Update the progress bar\n",
    "            history.append(loss.detach().numpy().mean())\n",
    "            plotter.plot(history)\n",
    "\n",
    "            # Update the model with the changed weights!\n",
    "            if iter % 100 == 0:\n",
    "                torch.save(model.state_dict(), checkpoint_prefix)\n",
    "\n",
    "    # Save the trained model and the weights\n",
    "    torch.save(model.state_dict(), checkpoint_prefix)\n",
    "\n",
    "if(inference):\n",
    "\n",
    "    ### Prediction of a generated song ###\n",
    "\n",
    "    def generate_text(model, start_string, generation_length=1000):\n",
    "        # Evaluation step (generating ABC text using the learned RNN model)\n",
    "\n",
    "        input_eval = [char2idx[s] for s in start_string]\n",
    "        input_eval = np.expand_dims(input_eval, axis=0)\n",
    "\n",
    "        # Empty string to store our results\n",
    "        text_generated = []\n",
    "\n",
    "        # Here batch size == 1\n",
    "        '''\n",
    "        for layer in model.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        '''\n",
    "        hn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "        cn = torch.zeros(1, 1, rnn_units)  # [num_layers*num_directions,batch,hidden_size]\n",
    "\n",
    "        tqdm._instances.clear()\n",
    "\n",
    "        for i in tqdm(range(generation_length)):\n",
    "            predictions, (hn, cn) = model(torch.tensor(input_eval), hn, cn)\n",
    "\n",
    "            # Remove the batch dimension\n",
    "            # predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = torch.squeeze(predictions, dim=0)\n",
    "\n",
    "            # predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "            predicted_id = torch.distributions.categorical.Categorical(logits=predictions).sample()[0].numpy()\n",
    "            # predicted_id = torch.distributions.categorical.Categorical(logits=predictions)\n",
    "\n",
    "            # Pass the prediction along with the previous hidden state\n",
    "            #   as the next inputs to the model\n",
    "            input_eval = np.expand_dims(np.array([predicted_id]), axis=0)\n",
    "\n",
    "            '''add the predicted character to the generated text!'''\n",
    "            # Hint: consider what format the prediction is in vs. the output\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "        return (start_string + ''.join(text_generated))\n",
    "\n",
    "    # Restore the model weights for the last checkpoint after training\n",
    "    model = MyLSTM(vocab_size, embedding_dim, rnn_units, batch_size, seq_length)\n",
    "    model.load_state_dict(torch.load(checkpoint_prefix))\n",
    "    model.eval()\n",
    "    print(model)\n",
    "\n",
    "    '''Use the model and the function defined above to generate ABC format text of length 1000!\n",
    "        As you may notice, ABC files start with \"X\" - this may be a good start string.'''\n",
    "    generated_text = generate_text(model, start_string=\"X\", generation_length=1000)\n",
    "\n",
    "    generated_songs = extract_song_snippet(generated_text)\n",
    "\n",
    "    for i, song in enumerate(generated_songs):\n",
    "        # could be incorrect ABC notational syntax, save the ABC file anyway...\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(\"Generated song\", i)\n",
    "        n = \"gan_song_{}\".format(i)\n",
    "        basename = os.path.join(op, save_song_to_abc(song, filename=n))\n",
    "        abc2wav(basename + '.abc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}